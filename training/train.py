"""
Training loop for Pokemon TCG AI.

Reads trajectory binary files generated by TypeScript self-play,
trains the policy-value network on GPU (MPS), and exports weights.

Binary trajectory format (per file):
  Header: [num_steps: uint32]
  Per step:
    state: 501 × f32
    num_actions: uint32
    action_features: num_actions × 54 × f32
    policy_target: num_actions × f32
    value_target: f32
"""

import argparse
import glob
import os
import struct
import time

import numpy as np
import torch
import torch.nn.functional as F

from model import PolicyValueNetwork, STATE_SIZE, ACTION_SIZE, export_weights, load_weights


def read_trajectory(path):
    """Read a binary trajectory file into training samples."""
    samples = []
    with open(path, 'rb') as f:
        data = f.read()

    offset = 0
    num_steps = struct.unpack_from('<I', data, offset)[0]
    offset += 4

    for _ in range(num_steps):
        # State: 501 floats
        state = np.frombuffer(data, dtype=np.float32, count=STATE_SIZE, offset=offset)
        offset += STATE_SIZE * 4

        # Num actions
        num_actions = struct.unpack_from('<I', data, offset)[0]
        offset += 4

        # Action features: num_actions × 54 floats
        action_feats = np.frombuffer(
            data, dtype=np.float32, count=num_actions * ACTION_SIZE, offset=offset
        ).reshape(num_actions, ACTION_SIZE)
        offset += num_actions * ACTION_SIZE * 4

        # Policy target: num_actions floats
        policy_target = np.frombuffer(
            data, dtype=np.float32, count=num_actions, offset=offset
        )
        offset += num_actions * 4

        # Value target: 1 float
        value_target = struct.unpack_from('<f', data, offset)[0]
        offset += 4

        samples.append({
            'state': state.copy(),
            'action_features': action_feats.copy(),
            'policy_target': policy_target.copy(),
            'value_target': value_target,
            'num_actions': num_actions,
        })

    return samples


def load_all_trajectories(data_dir):
    """Load all trajectory files from a directory."""
    # Search both direct .bin files and iter_*/ subdirs (replay buffer)
    files = sorted(glob.glob(os.path.join(data_dir, '*.bin')))
    files += sorted(glob.glob(os.path.join(data_dir, 'iter_*', '*.bin')))
    all_samples = []
    for f in files:
        try:
            samples = read_trajectory(f)
            all_samples.extend(samples)
        except Exception as e:
            print(f'Warning: failed to read {f}: {e}')
    return all_samples


def collate_batch(samples, device):
    """Collate a list of samples into padded batch tensors."""
    batch_size = len(samples)
    max_actions = max(s['num_actions'] for s in samples)

    states = torch.zeros(batch_size, STATE_SIZE, device=device)
    action_features = torch.zeros(batch_size, max_actions, ACTION_SIZE, device=device)
    action_mask = torch.zeros(batch_size, max_actions, device=device)
    policy_targets = torch.zeros(batch_size, max_actions, device=device)
    value_targets = torch.zeros(batch_size, device=device)

    for i, s in enumerate(samples):
        n = s['num_actions']
        states[i] = torch.from_numpy(s['state'])
        action_features[i, :n] = torch.from_numpy(s['action_features'])
        action_mask[i, :n] = 1.0
        policy_targets[i, :n] = torch.from_numpy(s['policy_target'])
        value_targets[i] = s['value_target']

    return states, action_features, action_mask, policy_targets, value_targets


def train_epoch(model, optimizer, samples, batch_size, device, entropy_coef=0.01):
    """Train one epoch over all samples."""
    model.train()
    indices = np.random.permutation(len(samples))
    total_loss = 0
    total_policy_loss = 0
    total_value_loss = 0
    total_entropy = 0
    num_batches = 0

    for start in range(0, len(indices), batch_size):
        end = min(start + batch_size, len(indices))
        batch_indices = indices[start:end]
        batch_samples = [samples[i] for i in batch_indices]

        states, action_feats, mask, policy_targets, value_targets = collate_batch(
            batch_samples, device
        )

        # Forward pass
        logits, values = model(states, action_feats, mask)

        # Policy loss: cross-entropy between target policy and network policy
        # logits are masked to -inf for invalid actions, so log_softmax
        # produces -inf for those positions. Use mask to avoid 0 * -inf = NaN.
        log_probs = F.log_softmax(logits, dim=-1)
        log_probs_safe = log_probs.masked_fill(mask == 0, 0.0)
        policy_loss = -(policy_targets * log_probs_safe).sum(dim=-1).mean()

        # Value loss: MSE
        value_loss = F.mse_loss(values, value_targets)

        # Entropy bonus (encourage exploration)
        probs = F.softmax(logits, dim=-1)
        probs_safe = probs.masked_fill(mask == 0, 0.0)
        entropy = -(probs_safe * log_probs_safe).sum(dim=-1).mean()

        # Total loss (2x value weight: value head has its own encoder now)
        loss = policy_loss + 2.0 * value_loss - entropy_coef * entropy

        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()

        total_loss += loss.item()
        total_policy_loss += policy_loss.item()
        total_value_loss += value_loss.item()
        total_entropy += entropy.item()
        num_batches += 1

    return {
        'loss': total_loss / max(num_batches, 1),
        'policy_loss': total_policy_loss / max(num_batches, 1),
        'value_loss': total_value_loss / max(num_batches, 1),
        'entropy': total_entropy / max(num_batches, 1),
    }


def main():
    parser = argparse.ArgumentParser(description='Train Pokemon TCG AI')
    parser.add_argument('--data', type=str, required=True, help='Directory with trajectory .bin files')
    parser.add_argument('--weights', type=str, default=None, help='Path to load initial weights from')
    parser.add_argument('--output', type=str, required=True, help='Path to save trained weights')
    parser.add_argument('--epochs', type=int, default=4, help='Number of training epochs')
    parser.add_argument('--batch-size', type=int, default=512, help='Batch size')
    parser.add_argument('--lr', type=float, default=3e-4, help='Learning rate')
    parser.add_argument('--entropy-coef', type=float, default=0.01, help='Entropy bonus coefficient')
    args = parser.parse_args()

    # Device
    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
    print(f'Device: {device}')

    # Model
    model = PolicyValueNetwork().to(device)
    print(f'Parameters: {model.count_parameters():,}')

    # Load existing weights if provided
    if args.weights and os.path.exists(args.weights):
        load_weights(model, args.weights)
        model = model.to(device)

    # Optimizer
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-4)

    # Load data
    print(f'Loading trajectories from {args.data}...')
    samples = load_all_trajectories(args.data)
    print(f'Loaded {len(samples)} training samples')

    if len(samples) == 0:
        print('No training data found!')
        return

    # Train
    for epoch in range(args.epochs):
        t0 = time.time()
        metrics = train_epoch(
            model, optimizer, samples, args.batch_size, device, args.entropy_coef
        )
        elapsed = time.time() - t0
        print(
            f'Epoch {epoch+1}/{args.epochs} | '
            f'loss={metrics["loss"]:.4f} | '
            f'policy={metrics["policy_loss"]:.4f} | '
            f'value={metrics["value_loss"]:.4f} | '
            f'entropy={metrics["entropy"]:.4f} | '
            f'{elapsed:.1f}s'
        )

    # Export weights
    os.makedirs(os.path.dirname(args.output) or '.', exist_ok=True)
    export_weights(model, args.output)
    print(f'Training complete. Weights saved to {args.output}')


if __name__ == '__main__':
    main()
