"""
Quick imitation learning from heuristic AI.

Trains the policy-value network to imitate the heuristic's action choices.
This proves the pipeline works and produces a model with basic intelligence
in ~5 minutes on MPS GPU.

Reads data generated by scripts/quick-train.ts (imitation trajectories).
"""

import argparse
import os
import time

import numpy as np
import torch
import torch.nn.functional as F

from model import PolicyValueNetwork, STATE_SIZE, ACTION_SIZE, export_weights, load_weights
from train import load_all_trajectories, collate_batch, train_epoch


def main():
    parser = argparse.ArgumentParser(description='Quick imitation learning from heuristic')
    parser.add_argument('--data', type=str, default='data/imitation', help='Imitation data directory')
    parser.add_argument('--output', type=str, default='models/latest_weights.json', help='Output weights path')
    parser.add_argument('--epochs', type=int, default=100, help='Training epochs')
    parser.add_argument('--batch-size', type=int, default=256, help='Batch size')
    parser.add_argument('--lr', type=float, default=1e-3, help='Learning rate')
    args = parser.parse_args()

    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
    print(f'Device: {device}')

    model = PolicyValueNetwork().to(device)
    print(f'Parameters: {model.count_parameters():,}')

    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)

    # Load imitation data
    print(f'Loading imitation data from {args.data}...')
    samples = load_all_trajectories(args.data)
    print(f'Loaded {len(samples)} training samples')

    if len(samples) == 0:
        print('No training data found! Run scripts/quick-train.ts first.')
        return

    # Train
    best_loss = float('inf')
    for epoch in range(args.epochs):
        t0 = time.time()
        metrics = train_epoch(
            model, optimizer, samples, args.batch_size, device,
            entropy_coef=0.005,  # lower entropy for imitation (follow teacher closely)
        )
        scheduler.step()
        elapsed = time.time() - t0

        if (epoch + 1) % 10 == 0 or epoch == 0:
            print(
                f'Epoch {epoch+1}/{args.epochs} | '
                f'loss={metrics["loss"]:.4f} | '
                f'policy={metrics["policy_loss"]:.4f} | '
                f'value={metrics["value_loss"]:.4f} | '
                f'entropy={metrics["entropy"]:.4f} | '
                f'lr={scheduler.get_last_lr()[0]:.6f} | '
                f'{elapsed:.1f}s'
            )

        if metrics['loss'] < best_loss:
            best_loss = metrics['loss']

    # Export weights
    os.makedirs(os.path.dirname(args.output) or '.', exist_ok=True)
    export_weights(model, args.output)
    print(f'\nQuick training complete!')
    print(f'Best loss: {best_loss:.4f}')
    print(f'Weights saved to {args.output}')


if __name__ == '__main__':
    main()
